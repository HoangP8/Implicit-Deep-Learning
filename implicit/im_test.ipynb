{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6971,  0.2575,  0.0000, -0.8757,  0.0000],\n",
      "        [-0.5603, -0.0000, -0.9305, -0.0779, -0.0124],\n",
      "        [-0.0000,  0.0000, -0.4501,  0.0000, -1.6561],\n",
      "        [-0.0000, -0.8902, -0.0000,  0.3090, -0.1272],\n",
      "        [-0.1076, -0.0000, -0.3441,  0.0000, -1.0732]])\n",
      "tensor([[ 0.3857,  0.0000,  0.0000, -0.5643,  0.0000],\n",
      "        [-0.2899, -0.0000, -0.6601, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.9500],\n",
      "        [-0.0000, -0.7648, -0.0000,  0.1835, -0.0017],\n",
      "        [-0.0000, -0.0000, -0.1105,  0.0000, -0.8395]])\n",
      "Are the original and optimized results the same? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create a 5x5 matrix A\n",
    "A = torch.randn(5, 5)  # Random values in a 5x5 matrix\n",
    "\n",
    "# Original code implementation\n",
    "def original_forward(A):\n",
    "    v = 0.95\n",
    "    A_np = A.clone().detach().cpu().numpy()\n",
    "    row_sums = np.abs(A_np).sum(axis=-1)\n",
    "\n",
    "    for idx in np.where(row_sums > v)[0]:\n",
    "        row = A_np[idx, :]\n",
    "        row_sign = np.sign(row)\n",
    "        row_abs = np.abs(row)\n",
    "        sorted_row = np.sort(row_abs)\n",
    "\n",
    "        s = np.sum(sorted_row) - v\n",
    "        l = float(len(sorted_row))\n",
    "        for i in range(len(sorted_row)):\n",
    "            if s / l > sorted_row[i]:\n",
    "                s -= sorted_row[i]\n",
    "                l -= 1\n",
    "            else:\n",
    "                break\n",
    "        alpha = s / l\n",
    "        A_np[idx, :] = row_sign * np.maximum(row_abs - alpha, 0)\n",
    "\n",
    "    return torch.tensor(A_np, dtype=A.dtype, device=A.device)\n",
    "\n",
    "import torch\n",
    "\n",
    "def fully_vectorized_forward(A, v=0.95):\n",
    "    # Step 1: Compute the row sums\n",
    "    row_sums = A.abs().sum(dim=1)\n",
    "\n",
    "    # Step 2: Mask the rows where sum is greater than v\n",
    "    mask = row_sums > v\n",
    "    A_masked = A[mask]\n",
    "\n",
    "    # Step 3: Sort the absolute values of the selected rows\n",
    "    row_abs = A_masked.abs()\n",
    "    sorted_row_abs, sorted_indices = torch.sort(row_abs, dim=1)\n",
    "\n",
    "    # Step 4: Compute the cumulative sum and find the index to cut\n",
    "    cumulative_sum = sorted_row_abs.cumsum(dim=1)\n",
    "    cut_idx = (cumulative_sum > (cumulative_sum[:, -1:] - v)).int().argmax(dim=1)\n",
    "\n",
    "    # Step 5: Compute alpha and subtract from the original rows\n",
    "    alpha = (cumulative_sum[torch.arange(cut_idx.size(0)), cut_idx] - v) / (cut_idx.float() + 1)\n",
    "\n",
    "    # Step 6: Rebuild the rows using the sign and maximum operation\n",
    "    row_sign = torch.sign(A_masked)\n",
    "    result_rows = row_sign * torch.maximum(row_abs - alpha.unsqueeze(1), torch.tensor(0.0, device=A.device))\n",
    "\n",
    "    # Step 7: Reassign the updated rows back to the matrix\n",
    "    A[mask] = result_rows\n",
    "\n",
    "    return A\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test the original and optimized versions\n",
    "\n",
    "A = torch.randn(5, 5)\n",
    "\n",
    "# Call the fully vectorized forward method\n",
    "A_vectorized = fully_vectorized_forward(A.clone())\n",
    "print(A_vectorized)\n",
    "\n",
    "# Optionally, compare with your original function (if available) to ensure they are the same\n",
    "A_original = original_forward(A.clone())\n",
    "print(A_original)\n",
    "\n",
    "# Check if the results are the same\n",
    "result = torch.allclose(A_vectorized, A_original)\n",
    "print(\"Are the original and optimized results the same?\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
      "        [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
      "        [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
      "        [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
      "        [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]])\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create a 5x5 matrix A\n",
    "A = torch.randn(5, 5) \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67841846 0.9007172  1.4872841  1.9269153  2.105521  ]\n",
      "0.67841846\n",
      "0.9007172\n",
      "1.4872841\n",
      "[0.04306748 0.7521353  1.2345449  1.604667   1.648723  ]\n",
      "0.043067478\n",
      "0.7521353\n",
      "[0.39247864 0.5594302  0.7278813  0.7688389  1.4036071 ]\n",
      "0.39247864\n",
      "0.5594302\n",
      "[0.15959747 0.43958926 0.49739754 0.7624454  1.6423169 ]\n",
      "0.15959747\n",
      "0.43958926\n",
      "0.49739754\n",
      "[0.75813115 0.80080056 1.0783176  1.2791244  1.6806206 ]\n",
      "0.75813115\n",
      "0.80080056\n",
      "[0.04175949 0.23162432 0.6104665  1.2964228  1.3347378 ]\n",
      "0.04175949\n",
      "0.23162432\n",
      "0.6104665\n",
      "[0.07802387 0.2515753  0.8598585  0.87123615 1.3846737 ]\n",
      "0.07802387\n",
      "0.2515753\n",
      "[0.48799172 0.52580875 0.7359928  0.81400764 1.191369  ]\n",
      "0.48799172\n",
      "0.52580875\n",
      "[0.06347727 0.09780689 0.6756149  0.83712465 0.92239004]\n",
      "0.06347727\n",
      "0.09780689\n",
      "[0.70781225 1.1845374  1.2024456  1.3835493  1.844594  ]\n",
      "0.70781225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create a 5x5 matrix A\n",
    "A = torch.randn(10, 5)  # Random values in a 5x5 matrix\n",
    "\n",
    "# Original code implementation\n",
    "def original_forward(A):\n",
    "    v = 0.95\n",
    "    A_np = A.clone().detach().cpu().numpy()\n",
    "    row_sums = np.abs(A_np).sum(axis=-1)\n",
    "    # print(row_sums)\n",
    "    \n",
    "    \n",
    "    for idx in np.where(row_sums > v)[0]:\n",
    "        # print(idx)\n",
    "        row = A_np[idx, :]\n",
    "        # print(row)\n",
    "        row_sign = np.sign(row)\n",
    "        row_abs = np.abs(row)\n",
    "        # print(row_abs)\n",
    "        sorted_row = np.sort(row_abs)\n",
    "        print(sorted_row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        s = np.sum(sorted_row) - v\n",
    "        # print(s)\n",
    "        l = float(len(sorted_row))\n",
    "        # print(l)\n",
    "        # print(s/l)\n",
    "        m = 0\n",
    "        for i in range(len(sorted_row)):\n",
    "            if s / l > sorted_row[i]:\n",
    "                # print(\"base\")\n",
    "                # print(s/l)\n",
    "                # print(\"matrix\")\n",
    "                print(sorted_row[i])\n",
    "                m += 1\n",
    "                s -= sorted_row[i]\n",
    "                l -= 1\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        # print(m)\n",
    "        # print(s)\n",
    "        # print(l)\n",
    "        alpha = s / l\n",
    "        A_np[idx, :] = row_sign * np.maximum(row_abs - alpha, 0)\n",
    "\n",
    "    return torch.tensor(A_np, dtype=A.dtype, device=A.device)\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test the original and optimized versions\n",
    "\n",
    "A = torch.randn(10, 5)\n",
    "\n",
    "# Optionally, compare with your original function (if available) to ensure they are the same\n",
    "A_original = original_forward(A.clone())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = A.abs().sum(dim=1)\n",
    "mask = row_sums > v\n",
    "A_masked = A[mask]\n",
    "row_abs = A_masked.abs()\n",
    "sorted_row_abs, _ = torch.sort(row_abs, dim=1)\n",
    "\n",
    "s = torch.sum(sorted_row_abs, 1) - v\n",
    "l = torch.tensor([float(len(sorted_row_abs))] * len(s))\n",
    "mask = sorted_row_abs < (s / l).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "# print(mask)\n",
    "num_elements = torch.sum(mask)\n",
    "print(num_elements)\n",
    "s -= np.sum(sorted_row_abs[:num_elements])\n",
    "l -= num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(6.1489)\n",
      "tensor([0.6784, 0.9007, 1.4873, 1.9269, 2.1055])\n",
      "1\n",
      "tensor(4.3331)\n",
      "tensor([0.0431, 0.7521, 1.2345, 1.6047, 1.6487])\n",
      "2\n",
      "tensor(2.9022)\n",
      "tensor([0.3925, 0.5594, 0.7279, 0.7688, 1.4036])\n",
      "3\n",
      "tensor(2.5513)\n",
      "tensor([0.1596, 0.4396, 0.4974, 0.7624, 1.6423])\n",
      "4\n",
      "tensor(4.6470)\n",
      "tensor([0.7581, 0.8008, 1.0783, 1.2791, 1.6806])\n",
      "5\n",
      "tensor(2.5650)\n",
      "tensor([0.0418, 0.2316, 0.6105, 1.2964, 1.3347])\n",
      "6\n",
      "tensor(2.4954)\n",
      "tensor([0.0780, 0.2516, 0.8599, 0.8712, 1.3847])\n",
      "7\n",
      "tensor(2.8052)\n",
      "tensor([0.4880, 0.5258, 0.7360, 0.8140, 1.1914])\n",
      "8\n",
      "tensor(1.6464)\n",
      "tensor([0.0635, 0.0978, 0.6756, 0.8371, 0.9224])\n",
      "9\n",
      "tensor(5.3729)\n",
      "tensor([0.7078, 1.1845, 1.2024, 1.3835, 1.8446])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Call the fully vectorized forward method\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m A_vectorized \u001b[38;5;241m=\u001b[39m \u001b[43mfully_vectorized_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(A_vectorized)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[126], line 41\u001b[0m, in \u001b[0;36mfully_vectorized_forward\u001b[0;34m(A, v)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sorted_row_abs[u,:])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# s[u] -= sorted_row_abs[u,:]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# print(s[u])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(num_elements_to_subtract)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Update s and l vectorized:\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m-\u001b[39m (sorted_row_abs \u001b[38;5;241m*\u001b[39m \u001b[43mmask\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m l \u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m-\u001b[39m num_elements_to_subtract\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(s)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(l)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def fully_vectorized_forward(A, v=0.95):\n",
    "    # Step 1: Compute the row sums\n",
    "    row_sums = A.abs().sum(dim=1)\n",
    "    # print(row_sums)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Step 2: Mask the rows where sum is greater than v\n",
    "    sorted_row_abs, _ = torch.sort(A[row_sums > v].abs(), dim=1)\n",
    "    # print(sorted_row_abs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4: Compute the cumulative sum and find the index to cut\n",
    "    s = torch.sum(sorted_row_abs, 1) - v\n",
    "    # print(s.shape)\n",
    "    l = torch.tensor([float(sorted_row_abs.shape[1])] * len(s))\n",
    "    alpha = s/l\n",
    "    # print(alpha)\n",
    "    # print(l)\n",
    "    # print((s/l).unsqueeze(1))\n",
    "    # mask = sorted_row_abs < (s / l).unsqueeze(1)\n",
    "    # print(mask)\n",
    "    \n",
    "    for u in range(sorted_row_abs.shape[0]):\n",
    "        # print(u)\n",
    "        # print(s[u])\n",
    "        # print(sorted_row_abs[u,:])\n",
    "        s[u] -= sorted_row_abs[u,:]\n",
    "        print(s[u])\n",
    "        \n",
    "        \n",
    "    # num_elements_to_subtract = mask.sum(dim=1)\n",
    "    # print(num_elements_to_subtract)\n",
    "    # Update s and l vectorized:\n",
    "    s = s - (sorted_row_abs * mask).sum(dim=1)\n",
    "    l = l - num_elements_to_subtract\n",
    "    # print(s)\n",
    "    # print(l)\n",
    "    return A\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Test the original and optimized versions\n",
    "\n",
    "A = torch.randn(10, 5)\n",
    "\n",
    "# Call the fully vectorized forward method\n",
    "A_vectorized = fully_vectorized_forward(A.clone())\n",
    "# print(A_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
