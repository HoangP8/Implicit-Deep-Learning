import random
import torch
import numpy as np
from torch import nn, Tensor
from typing import Any, Dict, Tuple


def set_seed(seed: int) -> None:
    """
    Set seed for reproducibility.
    """
    random.seed(seed) 
    np.random.seed(seed) 
    torch.manual_seed(seed) 
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


def print_model_size(model: nn.Module) -> None:
    """
    Print the total number of trainable parameters in a model.
    """
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total number of parameters: {total_params}")
    

def get_batch(
    data: Tensor,
    block_size: int,
    batch_size: int,
    device: torch.device
) -> Tuple[Tensor, Tensor]:
    """
    Create a batch of data for training or evaluation.
    
    Args:
        data (Tensor): Tensor containing sequence data.
        block_size (int): Number of data points in each sequence.
        batch_size (int): Number of sequences per batch.
        device (torch.device): Device to load the batch (CPU or GPU).
        
    Returns:
        Tuple[Tensor, Tensor]: 
            - x (Tensor): Input batch tensor of shape (batch_size, block_size).
            - y (Tensor): Target batch tensor of shape (batch_size, block_size).
    """
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i : i + block_size] for i in ix])
    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])
    return x.to(device), y.to(device)


@torch.no_grad()
def estimate_loss(
    model: nn.Module,
    data: Dict[str, Tensor],
    block_size: int,
    batch_size: int,
    device: torch.device,
    eval_iters: int
) -> Dict[str, float]:
    """
    Estimate the average loss of a model over specified iterations.
    
    Args:
        model (nn.Module): Model to evaluate.
        data (Dict[str, Tensor]): Data dictionary with 'train' and 'val' data.
        block_size (int): Sequence length.
        batch_size (int): Number of sequences in each batch.
        device (torch.device): Computation device.
        eval_iters (int): Number of iterations to average over.
        
    Returns:
        Dict[str, float]: 
            - 'train': Average loss on the training dataset.
            - 'val': Average loss on the validation dataset.
    """
    
    out = {}
    model.eval()
    for split in ["train", "val"]:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(data[split], block_size, batch_size, device)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


def generate_text(
    args: Any,
    idl_model: nn.Module,
    additional_data: Any,
    device: torch.device
) -> str:
    """
    Generate text using a pre-trained language model.
    
    Args:
        args (Any): Configurations from main.py containing attributes like 'dataset', 'max_new_tokens', and 'block_size'.
        idl_model (nn.Module): Trained implicit language model with a 'generate' method.
        additional_data (Any): Data needed for generation (e.g., tokenizer or encoding/decoding utilities).
        device (torch.device): Device for running the model.
        
    Returns:
        str: The text generated by the model.
    """
    if args.dataset == "tinyshakespeare":
        context = torch.zeros((1, 1), dtype=torch.long, device=device)
        generated_ids = idl_model.generate(context, args.max_new_tokens, args.block_size)[0].tolist()
        generated_text_idl = "".join([additional_data[i] for i in generated_ids])

    elif args.dataset == "tinystories":
        context = torch.tensor(additional_data.encode('\n'), dtype=torch.long, device=device).unsqueeze(0)
        generated_text_idl = additional_data.decode(
            idl_model.generate(context, args.max_new_tokens, args.block_size)[0].tolist()
        )

    elif args.dataset == "wikitext":
        context = torch.tensor(additional_data.encode_ordinary("\n"), dtype=torch.int, device=device).unsqueeze(0)
        generated_text_idl = additional_data.decode(
            idl_model.generate(context, args.max_new_tokens, args.block_size)[0].tolist()
        )

    else:
        raise NotImplementedError(f"Text generation for dataset {args.dataset} is not supported.")

    return generated_text_idl

