{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tutorial for Implicit Deep Learning and its variants\n",
    "\n",
    "This tutorial introduces the **Implicit Deep Learning** (IDL) framework using the `idl` package in 3 main parts:\n",
    "\n",
    "1. **Overview of IDL framework**\n",
    "    - Implcit Base model\n",
    "    - Implicit RNN\n",
    "    - Implicit Attention Head\n",
    "    - State-driven Implicit Model (SIM)\n",
    "2. **A toy example**\n",
    "    - Training Implicit Models and Their Variants\n",
    "3. **Custom Activation for Implicit framework**\n",
    "    - Customize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install idl\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of IDL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit Base model\n",
    "Implicit Deep Learning (IDL) models find a **fixed point solution** instead of explicitly stacking layers. \n",
    "\n",
    "Assume we have a dataset with $m$ data samples, each represented by $p$ input features. The model maps these inputs to a hidden state of dimension $n$ and produces predictions over $q$ output classes.\n",
    "\n",
    "Given an input matrix $ U \\in \\mathbb{R}^{p \\times m} $, the model maintains a state matrix $ X \\in \\mathbb{R}^{n \\times m} $ that satisfies:\n",
    "\n",
    "$$\n",
    "X = \\phi(AX + BU)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ A \\in \\mathbb{R}^{n \\times n} $, $ B \\in \\mathbb{R}^{n \\times p} $: learnable parameters,\n",
    "- $ \\phi: \\mathbb{R}^{n \\times m} \\to \\mathbb{R}^{n \\times m} $: the activation (e.g., ReLU, tanh, sigmoid),\n",
    "- $ U \\in \\mathbb{R}^{p \\times m} $:  the input matrix (each column is a data sample),\n",
    "- $ X \\in \\mathbb{R}^{n \\times m} $:  the hidden state.\n",
    "\n",
    "The model predicts the output matrix $ \\hat{Y} \\in \\mathbb{R}^{q \\times m} $ using:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = CX + DU\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ C \\in \\mathbb{R}^{q \\times n} $, $ D \\in \\mathbb{R}^{q \\times p} $: learnable parameters,\n",
    "- $ \\hat{Y} \\in \\mathbb{R}^{q \\times m} $: final model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a well-posed fixed-point solution, meaning that a **unique solution exists**, if:\n",
    "- The activation function is 1-Lipschitz: \n",
    "  $$\n",
    "  |\\phi(x) - \\phi(y)| \\leq |x - y|\n",
    "  $$\n",
    "\n",
    "- The matrix $ A $ satisfies:\n",
    "  $$\n",
    "  \\| A \\|_{\\infty} < 1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idl import ImplicitModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit RNN\n",
    "Instead of explicitly defining the hidden state update with a linear transformation, Implicit RNNs use an Implicit Base models to formulate the recurrence in a conventional RNN form. Assume we have a dataset with $m$ data samples, each consisting of a sequence length $T$. At each timestep, the input has $p$ features, and the model maintains a hidden state of dimension $n$.\n",
    "\n",
    "Given an input sequence $ U \\in \\mathbb{R}^{m \\times T \\times p} $, the implicit hidden state $ X_t \\in \\mathbb{R}^{m \\times n} $ satisfies the equilibrium equation:\n",
    "\n",
    "$$\n",
    "X_t = \\phi(AX_t + B [U_t, H_{t-1}]),\n",
    "$$\n",
    "\n",
    "followed by the output mapping equation, to return the RNN hidden state $H_t$:\n",
    "\n",
    "$$\n",
    "H_t = CX_t + DU_t,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ A \\in \\mathbb{R}^{n \\times n} $, $ B \\in \\mathbb{R}^{n \\times (p+n)} $, $C \\in \\mathbb{R}^{n \\times n}$, $D \\in \\mathbb{R}^{n \\times p}$: learnable parameters,\n",
    "- $ U_t \\in \\mathbb{R}^{m \\times p} $: the input at timestep $ t $,\n",
    "- $ X_t \\in \\mathbb{R}^{m \\times n} $: implicit hidden state at time step $t$ (solved via a fixed-point equation), \n",
    "- $ H_t $: the hidden state at timestep $ t $,\n",
    "- $ \\phi $: the activation (e.g., ReLU, tanh, sigmoid).\n",
    "\n",
    "Finally, a linear layer projects the final hidden state $H_T$ to the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idl import ImplicitRNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
