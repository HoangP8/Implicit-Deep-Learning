{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tutorial for Implicit Deep Learning\n",
    "\n",
    "This tutorial introduces the **Implicit Deep Learning** (IDL) framework using the `idl` package in 3 main parts:\n",
    "\n",
    "1. **A Simple Example**\n",
    "    - Implicit Model\n",
    "    - Implcit RNN\n",
    "    - State-driven Implicit Model (SIM)\n",
    "3. **Custom Activation for Implicit model**\n",
    "4. **Implicit model as a layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Simple Example\n",
    "\n",
    "This section provides a quick guide on how to use our package. With just a few lines of code, you can get started effortlessly.\n",
    "\n",
    "Before proceeding, please ensure you have installed the required packages by following the [installation](https://github.com/HoangP8/Implicit-Deep-Learning?tab=readme-ov-file#installation) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. `ImplicitModel`\n",
    "\n",
    "`ImplicitModel` is the most fundamental implicit model. For details on its parameters and the underlying intuition, please refer to the [documentation](link).\n",
    "\n",
    "In this example, we demonstrate how to use the model for a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5919\n",
      "Epoch [2/10], Loss: 1.0334\n",
      "Epoch [3/10], Loss: 0.4830\n",
      "Epoch [4/10], Loss: 0.1951\n",
      "Epoch [5/10], Loss: 0.1479\n",
      "Epoch [6/10], Loss: 0.1692\n",
      "Epoch [7/10], Loss: 0.1399\n",
      "Epoch [8/10], Loss: 0.0868\n",
      "Epoch [9/10], Loss: 0.0465\n",
      "Epoch [10/10], Loss: 0.0318\n",
      "Inference result: \n",
      " tensor([[-0.0525,  0.5056, -0.1804, -0.2234, -0.2438, -0.4717, -0.2398, -0.4559,\n",
      "          0.0045, -0.1295]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random input and output data\n",
    "x = torch.randn(5, 64).to(device)  # (batch_size=5, input_dim=64)\n",
    "y = torch.randn(5, 10).to(device)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64).to(device)\n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImplicitModel` has its forward and backward passes **fully packaged**, ensuring that the training and inference steps work **as normal**, with no additional modifications required. You only need to define the model with the appropriate `input_dim`, `output_dim`, and `hidden_dim`, and use it just like any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. `ImplicitRNN`\n",
    "\n",
    "`ImplicitRNN` uses an implicit layer to define recurrence within a standard RNN framework. For more details, please refer to the [documentation](link).\n",
    "\n",
    "Its usage is very similar to `ImplicitModel`. Below, we provide an example where the model learns to predict a single output from an input sequence in a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8179\n",
      "Epoch [2/10], Loss: 0.8017\n",
      "Epoch [3/10], Loss: 0.7861\n",
      "Epoch [4/10], Loss: 0.7708\n",
      "Epoch [5/10], Loss: 0.7557\n",
      "Epoch [6/10], Loss: 0.7392\n",
      "Epoch [7/10], Loss: 0.7199\n",
      "Epoch [8/10], Loss: 0.6989\n",
      "Epoch [9/10], Loss: 0.6883\n",
      "Epoch [10/10], Loss: 0.6879\n",
      "Inference result: tensor([[-0.5798]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitRNN\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random input and output sequence \n",
    "x = torch.randn(50, 20, 1).to(device)  # (batch_size=50, seq_len=20, input_dim=1)\n",
    "y = torch.randn(50, 1).to(device)  # (batch_size=50, output_dim=1)\n",
    "\n",
    "# Initialize the ImplicitRNN model\n",
    "model = ImplicitRNN(input_dim=1, output_dim=1, hidden_dim=10, implicit_hidden_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loops\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Inference step\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = torch.randn(1, 20, 1).to(device)\n",
    "    y_pred = model(x_test)\n",
    "    print(f\"Inference result: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. `SIM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset to mimic torchvision.datasets (with .data and .targets)\n",
    "class TorchvisionDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.targets = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.90s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuruacy: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from idl.sim import SIM\n",
    "from idl.sim.solvers import CVXSolver\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 5\n",
    "\n",
    "# Random training data\n",
    "x_train = torch.randn(100, 64).to(device) # (no_samples=100, input_dim=64)\n",
    "y_train = torch.randint(0, 10, (100,)).to(device) # 100 random class labels (0-9)\n",
    "train_loader = DataLoader(TorchvisionDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Random test data\n",
    "x_test = torch.randn(20, 64).to(device) # (no_samples=20, input_dim=64)\n",
    "y_test = torch.randint(0, 10, (20,)).to(device) # 20 random class labels (0-9)\n",
    "test_loader = DataLoader(TorchvisionDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Explicit model \n",
    "explicit_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 10)\n",
    ").to(device)\n",
    "\n",
    "# SIM model\n",
    "sim = SIM(device=device)\n",
    "\n",
    "# Define CrossEntropy loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(explicit_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train and evaluate the SIM model\n",
    "sim.train(solver=CVXSolver(), model=explicit_model, dataloader=train_loader)\n",
    "\n",
    "# After training, evaluate the SIM model on test data\n",
    "sim.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Activation for Implicit model\n",
    "The default activation of the Implicit model is ReLU. To override the implicit function you wish to use, just simply replace the `phi` and `dphi` (gradient of activation) methods. Below is an example of SiLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplicitFunctionInf: function to ensure wellposedness of Implicit model\n",
    "from idl import ImplicitModel, ImplicitFunctionInf \n",
    "import torch\n",
    "\n",
    "class ImplicitFunctionInfSiLU(ImplicitFunctionInf):\n",
    "    \"\"\"\n",
    "    An implicit function that uses the SiLU nonlinearity.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(X):\n",
    "        return X * torch.sigmoid(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def dphi(X):\n",
    "        grad = X.clone().detach()\n",
    "        sigmoid = torch.sigmoid(grad)\n",
    "        return sigmoid * (1 + grad * (1 - sigmoid))\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128,\n",
    "                      f=ImplicitFunctionInfSiLU)\n",
    "\n",
    "# train model normally after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit model as a layer\n",
    "Implicit Model can be integrated as a layer within larger models, allowing it to be trained as part of the overall network. The training process works normally, below is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7000\n",
      "Epoch [2/10], Loss: 0.3324\n",
      "Epoch [3/10], Loss: 0.1701\n",
      "Epoch [4/10], Loss: 0.0583\n",
      "Epoch [5/10], Loss: 0.0516\n",
      "Epoch [6/10], Loss: 0.0430\n",
      "Epoch [7/10], Loss: 0.0322\n",
      "Epoch [8/10], Loss: 0.0248\n",
      "Epoch [9/10], Loss: 0.0216\n",
      "Epoch [10/10], Loss: 0.0203\n",
      "Inference result: \n",
      " tensor([[-0.7536, -0.1237,  0.1082, -0.7727,  0.6030,  1.1026,  0.3494,  0.2714,\n",
      "          0.3646,  0.4176]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a larger model that includes ImplicitModel as a layer\n",
    "class MLPWithImplicit(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, implicit_hidden_dim, output_dim):\n",
    "        super(MLPWithImplicit, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.implicit_layer = ImplicitModel(input_dim=hidden_dim, output_dim=output_dim, hidden_dim=implicit_hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.implicit_layer(x)  # Pass through ImplicitModel\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Random input and output data\n",
    "x = torch.randn(5, 64).to(device)  # (batch_size=5, input_dim=64)\n",
    "y = torch.randn(5, 10).to(device)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPWithImplicit(input_dim=64, hidden_dim=128, implicit_hidden_dim=64, output_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64).to(device)  \n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
