{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tutorial for Implicit Deep Learning\n",
    "\n",
    "This tutorial introduces the **Implicit Deep Learning** (IDL) framework using the `idl` package in 3 main parts:\n",
    "\n",
    "1. **A Simple Example**\n",
    "    - Implicit Model\n",
    "    - Implcit RNN\n",
    "    - State-driven Implicit Model (SIM)\n",
    "3. **Custom Activation for Implicit model**\n",
    "4. **Implicit model as a layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Simple Example\n",
    "\n",
    "This section provides a quick guide on how to use our package. With just a few lines of code, you can get started effortlessly.\n",
    "\n",
    "Before proceeding, please ensure you have installed the required packages by following the [installation](https://github.com/HoangP8/Implicit-Deep-Learning?tab=readme-ov-file#installation) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. `ImplicitModel`\n",
    "\n",
    "`ImplicitModel` is the most fundamental implicit model. For details on its parameters and the underlying intuition, please refer to the [documentation](link).\n",
    "\n",
    "In this example, we demonstrate how to use the model for a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7182\n",
      "Epoch [2/10], Loss: 0.3191\n",
      "Epoch [3/10], Loss: 0.1070\n",
      "Epoch [4/10], Loss: 0.1025\n",
      "Epoch [5/10], Loss: 0.0644\n",
      "Epoch [6/10], Loss: 0.0446\n",
      "Epoch [7/10], Loss: 0.0357\n",
      "Epoch [8/10], Loss: 0.0261\n",
      "Epoch [9/10], Loss: 0.0170\n",
      "Epoch [10/10], Loss: 0.0130\n",
      "Inference result: \n",
      " tensor([[ 0.1920,  0.2060, -0.3569, -0.2839, -0.1086,  0.3988, -0.2308, -0.2174,\n",
      "         -0.0737,  0.0042]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "torch.manual_seed(42) # fix seed\n",
    "\n",
    "# Random input data\n",
    "x = torch.randn(5, 64)  # (batch_size=5, input_dim=64)\n",
    "\n",
    "# Random ground truth values\n",
    "y = torch.randn(5, 10)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64)  \n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImplicitModel` has its forward and backward passes **fully packaged**, ensuring that the training and inference steps work **as normal**, with no additional modifications required. You only need to define the model with the appropriate `input_dim`, `output_dim`, and `hidden_dim`, and use it just like any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. `ImplicitRNN`\n",
    "\n",
    "`ImplicitRNN` uses an implicit layer to define recurrence within a standard RNN framework. For more details, please refer to the [documentation](link).\n",
    "\n",
    "Its usage is very similar to `ImplicitModel`. Below, we provide an example where the model learns to predict a single output from an input sequence in a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0385\n",
      "Epoch [2/10], Loss: 1.0199\n",
      "Epoch [3/10], Loss: 1.0032\n",
      "Epoch [4/10], Loss: 0.9882\n",
      "Epoch [5/10], Loss: 0.9739\n",
      "Epoch [6/10], Loss: 0.9594\n",
      "Epoch [7/10], Loss: 0.9435\n",
      "Epoch [8/10], Loss: 0.9255\n",
      "Epoch [9/10], Loss: 0.9043\n",
      "Epoch [10/10], Loss: 0.8801\n",
      "Inference result: tensor([[-0.5701]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitRNN\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Random input sequence data\n",
    "x = torch.randn(50, 20, 1)  # (batch_size=50, seq_len=20, input_dim=1)\n",
    "\n",
    "# Random ground truth sequence values\n",
    "y = torch.randn(50, 1)  # (batch_size=50, output_dim=1)\n",
    "\n",
    "# Initialize the ImplicitRNN model\n",
    "model = ImplicitRNN(input_dim=1, output_dim=1, hidden_dim=10, implicit_hidden_dim=10)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loops\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Inference step\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = torch.randn(1, 20, 1)\n",
    "    y_pred = model(x_test)\n",
    "    print(f\"Inference result: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. `SIM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Activation for Implicit model\n",
    "The default activation of the Implicit model is ReLU. To override the implicit function you wish to use, just simply replace the `phi` and `dphi` (gradient of activation) methods. Below is an example of SiLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplicitFunctionInf: function to ensure wellposedness of Implicit model\n",
    "from idl import ImplicitModel, ImplicitFunctionInf \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ImplicitFunctionInfSiLU(ImplicitFunctionInf):\n",
    "    \"\"\"\n",
    "    An implicit function that uses the SiLU nonlinearity.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(X):\n",
    "        return X * torch.sigmoid(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def dphi(X):\n",
    "        grad = X.clone().detach()\n",
    "        sigmoid = torch.sigmoid(grad)\n",
    "        return sigmoid * (1 + grad * (1 - sigmoid))\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128,\n",
    "                      f=ImplicitFunctionInfSiLU)\n",
    "\n",
    "# train model normally after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit model as a layer\n",
    "Implicit Model can be integrated as a layer within larger models, allowing it to be trained as part of the overall network. The training process works normally, below is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7000\n",
      "Epoch [2/10], Loss: 0.3326\n",
      "Epoch [3/10], Loss: 0.1685\n",
      "Epoch [4/10], Loss: 0.0587\n",
      "Epoch [5/10], Loss: 0.0519\n",
      "Epoch [6/10], Loss: 0.0429\n",
      "Epoch [7/10], Loss: 0.0319\n",
      "Epoch [8/10], Loss: 0.0247\n",
      "Epoch [9/10], Loss: 0.0217\n",
      "Epoch [10/10], Loss: 0.0204\n",
      "Inference result: \n",
      " tensor([[-0.7506, -0.1255,  0.1125, -0.7755,  0.6034,  1.0982,  0.3507,  0.2699,\n",
      "          0.3629,  0.4228]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "# Define a larger model that includes ImplicitModel as a layer\n",
    "class MLPWithImplicit(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, implicit_hidden_dim, output_dim):\n",
    "        super(MLPWithImplicit, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.implicit_layer = ImplicitModel(input_dim=hidden_dim, output_dim=output_dim, hidden_dim=implicit_hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.implicit_layer(x)  # Pass through ImplicitModel\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(42) # fix seed\n",
    "\n",
    "# Random input data\n",
    "x = torch.randn(5, 64)  # (batch_size=5, input_dim=64)\n",
    "\n",
    "# Random ground truth values\n",
    "y = torch.randn(5, 10)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPWithImplicit(input_dim=64, hidden_dim=128, implicit_hidden_dim=64, output_dim=10)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64)  \n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
